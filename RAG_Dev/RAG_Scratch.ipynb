{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6249a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "chromadb        # Vector database for document storage\n",
    "openai          # LLM API access\n",
    "pypdf2          # PDF document processing\n",
    "python-docx     # Word document processing\n",
    "sentence-transformers  # Text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee28ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "def read_text_file(file_path: str):\n",
    "    \"\"\"Read content from a text file\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_pdf_file(file_path: str):\n",
    "    \"\"\"Read content from a PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx_file(file_path: str):\n",
    "    \"\"\"Read content from a Word document\"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfbe25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(file_path: str):\n",
    "    \"\"\"Read document content based on file extension\"\"\"\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    file_extension = file_extension.lower()\n",
    "\n",
    "    if file_extension == '.txt':\n",
    "        return read_text_file(file_path)\n",
    "    elif file_extension == '.pdf':\n",
    "        return read_pdf_file(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        return read_docx_file(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_extension}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f22bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text: str, chunk_size: int = 500):\n",
    "    \"\"\"Split text into chunks while preserving sentence boundaries\"\"\"\n",
    "    sentences = text.replace('\\n', ' ').split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "\n",
    "        # Ensure proper sentence ending\n",
    "        if not sentence.endswith('.'):\n",
    "            sentence += '.'\n",
    "\n",
    "        sentence_size = len(sentence)\n",
    "\n",
    "        # Check if adding this sentence would exceed chunk size\n",
    "        if current_size + sentence_size > chunk_size and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_size = sentence_size\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_size += sentence_size\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b114165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Initialize ChromaDB client with persistence\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "\n",
    "# Configure sentence transformer embeddings\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create or get existing collection\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"documents_collection\",\n",
    "    embedding_function=sentence_transformer_ef\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path: str):\n",
    "    \"\"\"Process a single document and prepare it for ChromaDB\"\"\"\n",
    "    try:\n",
    "        # Read the document\n",
    "        content = read_document(file_path)\n",
    "\n",
    "        # Split into chunks\n",
    "        chunks = split_text(content)\n",
    "\n",
    "        # Prepare metadata\n",
    "        file_name = os.path.basename(file_path)\n",
    "        metadatas = [{\"source\": file_name, \"chunk\": i} for i in range(len(chunks))]\n",
    "        ids = [f\"{file_name}_chunk_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "        return ids, chunks, metadatas\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_collection(collection, ids, texts, metadatas):\n",
    "    \"\"\"Add documents to collection in batches\"\"\"\n",
    "    if not texts:\n",
    "        return\n",
    "\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        end_idx = min(i + batch_size, len(texts))\n",
    "        collection.add(\n",
    "            documents=texts[i:end_idx],\n",
    "            metadatas=metadatas[i:end_idx],\n",
    "            ids=ids[i:end_idx]\n",
    "        )\n",
    "\n",
    "def process_and_add_documents(collection, folder_path: str):\n",
    "    \"\"\"Process all documents in a folder and add to collection\"\"\"\n",
    "    files = [os.path.join(folder_path, file) \n",
    "             for file in os.listdir(folder_path) \n",
    "             if os.path.isfile(os.path.join(folder_path, file))]\n",
    "\n",
    "    for file_path in files:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        ids, texts, metadatas = process_document(file_path)\n",
    "        add_to_collection(collection, ids, texts, metadatas)\n",
    "        print(f\"Added {len(texts)} chunks to collection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f81232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB collection (we'll cover this in detail in the next section)\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"documents_collection\",\n",
    "    embedding_function=sentence_transformer_ef\n",
    ")\n",
    "\n",
    "# Process and add documents from a folder\n",
    "folder_path = \"/docs\"\n",
    "process_and_add_documents(collection, folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01197a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(collection, query: str, n_results: int = 2):\n",
    "    \"\"\"Perform semantic search on the collection\"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    return results\n",
    "\n",
    "def get_context_with_sources(results):\n",
    "    \"\"\"Extract context and source information from search results\"\"\"\n",
    "    # Combine document chunks into a single context\n",
    "    context = \"\\n\\n\".join(results['documents'][0])\n",
    "\n",
    "    # Format sources with metadata\n",
    "    sources = [\n",
    "        f\"{meta['source']} (chunk {meta['chunk']})\" \n",
    "        for meta in results['metadatas'][0]\n",
    "    ]\n",
    "\n",
    "    return context, sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7888acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a search\n",
    "query = \"When was GreenGrow Innovations founded?\"\n",
    "results = semantic_search(collection, query)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df30fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_search_results(results):\n",
    "    \"\"\"Print formatted search results\"\"\"\n",
    "    print(\"\\nSearch Results:\\n\" + \"-\" * 50)\n",
    "\n",
    "    for i in range(len(results['documents'][0])):\n",
    "        doc = results['documents'][0][i]\n",
    "        meta = results['metadatas'][0][i]\n",
    "        distance = results['distances'][0][i]\n",
    "\n",
    "        print(f\"\\nResult {i + 1}\")\n",
    "        print(f\"Source: {meta['source']}, Chunk {meta['chunk']}\")\n",
    "        print(f\"Distance: {distance}\")\n",
    "        print(f\"Content: {doc}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Set your API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6143b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(context: str, conversation_history: str, query: str):\n",
    "    \"\"\"Generate a prompt combining context, history, and query\"\"\"\n",
    "    prompt = f\"\"\"Based on the following context and conversation history, \n",
    "    please provide a relevant and contextual response. If the answer cannot \n",
    "    be derived from the context, only use the conversation history or say \n",
    "    \"I cannot answer this based on the provided information.\"\n",
    "\n",
    "    Context from documents:\n",
    "    {context}\n",
    "\n",
    "    Previous conversation:\n",
    "    {conversation_history}\n",
    "\n",
    "    Human: {query}\n",
    "\n",
    "    Assistant:\"\"\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query: str, context: str, conversation_history: str = \"\"):\n",
    "    \"\"\"Generate a response using OpenAI with conversation history\"\"\"\n",
    "    prompt = get_prompt(context, conversation_history, query)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",  # or gpt-3.5-turbo for lower cost\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0,  # Lower temperature for more focused responses\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1367892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(collection, query: str, n_chunks: int = 2):\n",
    "    \"\"\"Perform RAG query: retrieve relevant chunks and generate answer\"\"\"\n",
    "    # Get relevant chunks\n",
    "    results = semantic_search(collection, query, n_chunks)\n",
    "    context, sources = get_context_with_sources(results)\n",
    "\n",
    "    # Generate response\n",
    "    response = generate_response(query, context)\n",
    "\n",
    "    return response, sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dda8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"When was GreenGrow Innovations founded?\"\n",
    "response, sources = rag_query(collection, query)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nQuery:\", query)\n",
    "print(\"\\nAnswer:\", response)\n",
    "print(\"\\nSources used:\")\n",
    "for source in sources:\n",
    "    print(f\"- {source}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2b16ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# In-memory conversation store\n",
    "conversations = {}\n",
    "\n",
    "def create_session():\n",
    "    \"\"\"Create a new conversation session\"\"\"\n",
    "    session_id = str(uuid.uuid4())\n",
    "    conversations[session_id] = []\n",
    "    return session_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b61ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(session_id: str, role: str, content: str):\n",
    "    \"\"\"Add a message to the conversation history\"\"\"\n",
    "    if session_id not in conversations:\n",
    "        conversations[session_id] = []\n",
    "\n",
    "    conversations[session_id].append({\n",
    "        \"role\": role,\n",
    "        \"content\": content,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "def get_conversation_history(session_id: str, max_messages: int = None):\n",
    "    \"\"\"Get conversation history for a session\"\"\"\n",
    "    if session_id not in conversations:\n",
    "        return []\n",
    "\n",
    "    history = conversations[session_id]\n",
    "    if max_messages:\n",
    "        history = history[-max_messages:]\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f46e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_history_for_prompt(session_id: str, max_messages: int = 5):\n",
    "    \"\"\"Format conversation history for inclusion in prompts\"\"\"\n",
    "    history = get_conversation_history(session_id, max_messages)\n",
    "    formatted_history = \"\"\n",
    "\n",
    "    for msg in history:\n",
    "        role = \"Human\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "        formatted_history += f\"{role}: {msg['content']}\\n\\n\"\n",
    "\n",
    "    return formatted_history.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72230caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextualize_query(query: str, conversation_history: str, client: OpenAI):\n",
    "    \"\"\"Convert follow-up questions into standalone queries\"\"\"\n",
    "    contextualize_prompt = \"\"\"Given a chat history and the latest user question \n",
    "    which might reference context in the chat history, formulate a standalone \n",
    "    question which can be understood without the chat history. Do NOT answer \n",
    "    the question, just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": contextualize_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Chat history:\\n{conversation_history}\\n\\nQuestion:\\n{query}\"}\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error contextualizing query: {str(e)}\")\n",
    "        return query  # Fallback to original query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f82e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(context, conversation_history, query):\n",
    "  prompt = f\"\"\"Based on the following context and conversation history, please provide a relevant and contextual response.\n",
    "    If the answer cannot be derived from the context, only use the conversation history or say \"I cannot answer this based on the provided information.\"\n",
    "\n",
    "    Context from documents:\n",
    "    {context}\n",
    "\n",
    "    Previous conversation:\n",
    "    {conversation_history}\n",
    "\n",
    "    Human: {query}\n",
    "\n",
    "    Assistant:\"\"\"\n",
    "  return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8086e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated generate response function with conversation history also passed for Chatbot Memory\n",
    "def generate_response(query: str, context: str, conversation_history: str = \"\"):\n",
    "    \"\"\"Generate a response using OpenAI with conversation history\"\"\"\n",
    "    prompt = get_prompt(context, conversation_history, query)\n",
    "    # print(prompt)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b03e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversational_rag_query(\n",
    "    collection,\n",
    "    query: str,\n",
    "    session_id: str,\n",
    "    n_chunks: int = 3\n",
    "):\n",
    "    \"\"\"Perform RAG query with conversation history\"\"\"\n",
    "    # Get conversation history\n",
    "    conversation_history = format_history_for_prompt(session_id)\n",
    "\n",
    "    # Handle follo up questions\n",
    "    query = contextualize_query(query, conversation_history, client)\n",
    "    print(\"Contextualized Query:\", query)\n",
    "\n",
    "    # Get relevant chunks\n",
    "    context, sources = get_context_with_sources(\n",
    "        semantic_search(collection, query, n_chunks)\n",
    "    )\n",
    "    print(\"Context:\", context)\n",
    "    print(\"Sources:\", sources)\n",
    "\n",
    "\n",
    "    response = generate_response(query, context, conversation_history)\n",
    "\n",
    "    # Add to conversation history\n",
    "    add_message(session_id, \"user\", query)\n",
    "    add_message(session_id, \"assistant\", response)\n",
    "\n",
    "    return response, sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e32149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new conversation session\n",
    "session_id = create_session()\n",
    "\n",
    "# First question\n",
    "query = \"When was GreenGrow Innovations founded?\"\n",
    "response, sources = conversational_rag_query(\n",
    "            collection,\n",
    "            query,\n",
    "            session_id\n",
    ")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5002035",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Where is it located?\"\n",
    "response, sources = conversational_rag_query(\n",
    "            collection,\n",
    "            query,\n",
    "            session_id\n",
    ")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
