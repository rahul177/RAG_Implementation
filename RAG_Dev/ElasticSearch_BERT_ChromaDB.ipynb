{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import chromadb\n",
    "from elasticsearch import Elasticsearch\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Elasticsearch\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "# Initialize BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize ChromaDB\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Function to generate embeddings using BERT\n",
    "def generate_embedding(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    return output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Indexing documents\n",
    "def index_document(doc_id, text):\n",
    "    # Generate embedding\n",
    "    embedding = generate_embedding(text)\n",
    "\n",
    "    # Index in Elasticsearch\n",
    "    es.index(index=\"documents\", id=doc_id, body={\"text\": text})\n",
    "\n",
    "    # Index in ChromaDB\n",
    "    client.insert(id=doc_id, embedding=embedding, metadata={\"text\": text})\n",
    "\n",
    "# Searching query\n",
    "def hybrid_search(query, top_n=10):\n",
    "    # Generate query embedding\n",
    "    query_embedding = generate_embedding(query)\n",
    "\n",
    "    # Elasticsearch keyword search\n",
    "    es_results = es.search(index=\"documents\", body={\n",
    "        \"query\": {\"match\": {\"text\": query}},\n",
    "        \"size\": top_n\n",
    "    })\n",
    "    es_scores = {hit[\"_id\"]: hit[\"_score\"] for hit in es_results[\"hits\"][\"hits\"]}\n",
    "\n",
    "    # ChromaDB semantic search\n",
    "    chroma_results = client.query(query_embedding, n_results=top_n)\n",
    "    chroma_scores = {res[\"id\"]: res[\"score\"] for res in chroma_results}\n",
    "\n",
    "    # Combine scores\n",
    "    combined_scores = {}\n",
    "    for doc_id in set(es_scores.keys()).union(chroma_scores.keys()):\n",
    "        combined_scores[doc_id] = 0.5 * es_scores.get(doc_id, 0) + 0.5 * chroma_scores.get(doc_id, 0)\n",
    "\n",
    "    # Rank results\n",
    "    ranked_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ranked_results[:top_n]\n",
    "\n",
    "# Example usage\n",
    "index_document(\"1\", \"This is a sample document.\")\n",
    "index_document(\"2\", \"Another document about machine learning.\")\n",
    "results = hybrid_search(\"sample machine learning\")\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
