{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ff0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm_response = llm.invoke(\"Tell me a joke\")\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "    output_parser = StrOutputParser()\n",
    "    chain = llm | output_parser\n",
    "    result = chain.invoke(\"Tell me a joke\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba412580",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from typing import List\n",
    "    from pydantic import BaseModel, Field\n",
    "\n",
    "    class MobileReview(BaseModel):\n",
    "        phone_model: str = Field(description=\"Name and model of the phone\")\n",
    "        rating: float = Field(description=\"Overall rating out of 5\")\n",
    "        pros: List[str] = Field(description=\"List of positive aspects\")\n",
    "        cons: List[str] = Field(description=\"List of negative aspects\")\n",
    "        summary: str = Field(description=\"Brief summary of the review\")\n",
    "\n",
    "    review_text = \"\"\"\n",
    "    Just got my hands on the new Galaxy S21 and wow, this thing is slick! The screen is gorgeous,\n",
    "    colors pop like crazy. Camera's insane too, especially at night - my Insta game's never been\n",
    "    stronger. Battery life's solid, lasts me all day no problem.\n",
    "    Not gonna lie though, it's pretty pricey. And what's with ditching the charger? C'mon Samsung.\n",
    "    Also, still getting used to the new button layout, keep hitting Bixby by mistake.\n",
    "    Overall, I'd say it's a solid 4 out of 5. Great phone, but a few annoying quirks keep it from\n",
    "    being perfect. If you're due for an upgrade, definitely worth checking out!\n",
    "    \"\"\"\n",
    "\n",
    "    structured_llm = llm.with_structured_output(MobileReview)\n",
    "    output = structured_llm.invoke(review_text)\n",
    "    print(output)\n",
    "    print(output.pros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "    chain = prompt | llm | output_parser\n",
    "    result = chain.invoke({\"topic\": \"programming\"})\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e5279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant that tells jokes.\"),\n",
    "        HumanMessage(content=\"Tell me about programming\")\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    print(response)\n",
    "\n",
    "    template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
    "        (\"human\", \"Tell me about {topic}\")\n",
    "    ])\n",
    "    chain = template | llm\n",
    "    response = chain.invoke({\"topic\": \"programming\"})\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "def load_documents(folder_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif filename.endswith('.docx'):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {filename}\")\n",
    "            continue\n",
    "        documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "folder_path = \"/content/docs\"\n",
    "documents = load_documents(folder_path)\n",
    "print(f\"Loaded {len(documents)} documents from the folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3fcbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split the documents into {len(splits)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edcf02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(splits[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0716e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "document_embeddings = embeddings.embed_documents([split.page_content for split in splits])\n",
    "print(f\"Created embeddings for {len(document_embeddings)} document chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d69b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "document_embeddings = embedding_function.embed_documents([split.page_content for split in splits])\n",
    "print(document_embeddings[0][:5])  # Printing first 5 elements of the first embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d29ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "collection_name = \"my_collection\"\n",
    "vectorstore = Chroma.from_documents(\n",
    "    collection_name=collection_name,\n",
    "    documents=splits,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "print(\"Vector store created and persisted to './chroma_db'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"When was GreenGrow Innovations founded?\"\n",
    "search_results = vectorstore.similarity_search(query, k=2)\n",
    "print(f\"\\nTop 2 most relevant chunks for the query: '{query}'\\n\")\n",
    "for i, result in enumerate(search_results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Content: {result.page_content}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f14726",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "retriever_results = retriever.invoke(\"When was GreenGrow Innovations founded?\")\n",
    "print(retriever_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def docs2str(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2155a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When was GreenGrow Innovations founded?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5128bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"\n",
    "Given a chat history and the latest user question\n",
    "which might reference context in the chat history,\n",
    "formulate a standalone question which can be understood\n",
    "without the chat history. Do NOT answer the question,\n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "contextualize_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
    "print(contextualize_chain.invoke({\"input\": \"Where is it headquartered?\", \"chat_history\": []}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6aca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question.\"),\n",
    "    (\"system\", \"Context: {context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e63d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "question1 = \"When was GreenGrow Innovations founded?\"\n",
    "answer1 = rag_chain.invoke({\"input\": question1, \"chat_history\": chat_history})['answer']\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question1),\n",
    "    AIMessage(content=answer1)\n",
    "])\n",
    "\n",
    "print(f\"Human: {question1}\")\n",
    "print(f\"AI: {answer1}\\n\")\n",
    "\n",
    "question2 = \"Where is it headquartered?\"\n",
    "answer2 = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history})['answer']\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question2),\n",
    "    AIMessage(content=answer2)\n",
    "])\n",
    "\n",
    "print(f\"Human: {question2}\")\n",
    "print(f\"AI: {answer2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d048908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "DB_NAME = \"rag_app.db\"\n",
    "\n",
    "def get_db_connection():\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    conn.row_factory = sqlite3.Row\n",
    "    return conn\n",
    "\n",
    "def create_application_logs():\n",
    "    conn = get_db_connection()\n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS application_logs\n",
    "    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    session_id TEXT,\n",
    "    user_query TEXT,\n",
    "    gpt_response TEXT,\n",
    "    model TEXT,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
    "    conn.close()\n",
    "\n",
    "def insert_application_logs(session_id, user_query, gpt_response, model):\n",
    "    conn = get_db_connection()\n",
    "    conn.execute('INSERT INTO application_logs (session_id, user_query, gpt_response, model) VALUES (?, ?, ?, ?)',\n",
    "                 (session_id, user_query, gpt_response, model))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def get_chat_history(session_id):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
    "    messages = []\n",
    "    for row in cursor.fetchall():\n",
    "        messages.extend([\n",
    "            {\"role\": \"human\", \"content\": row['user_query']},\n",
    "            {\"role\": \"ai\", \"content\": row['gpt_response']}\n",
    "        ])\n",
    "    conn.close()\n",
    "    return messages\n",
    "\n",
    "# Initialize the database\n",
    "create_application_logs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c80f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for a new user\n",
    "session_id = str(uuid.uuid4())\n",
    "question = \"What is GreenGrow Innovations?\"\n",
    "chat_history = get_chat_history(session_id)\n",
    "answer = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})['answer']\n",
    "insert_application_logs(session_id, question, answer, \"gpt-3.5-turbo\")\n",
    "print(f\"Human: {question}\")\n",
    "print(f\"AI: {answer}\\n\")\n",
    "\n",
    "# Example of a follow-up question\n",
    "question2 = \"What was their first product?\"\n",
    "chat_history = get_chat_history(session_id)\n",
    "answer2 = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history})['answer']\n",
    "insert_application_logs(session_id, question2, answer2, \"gpt-3.5-turbo\")\n",
    "print(f\"Human: {question2}\")\n",
    "print(f\"AI: {answer2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b87c3",
   "metadata": {},
   "source": [
    "FASTAPI Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd051e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Setup\n",
    "mkdir rag-fastapi-project\n",
    "cd rag-fastapi-project\n",
    "\n",
    "# Requirements.txt\n",
    "\n",
    "langchain\n",
    "langchain-openai\n",
    "langchain-core\n",
    "langchain_community\n",
    "docx2txt\n",
    "pypdf\n",
    "langchain_chroma\n",
    "python-multipart\n",
    "fastapi\n",
    "uvicorn\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "rag-fastapi-project/\n",
    "│\n",
    "├── main.py\n",
    "├── chroma_utils.py\n",
    "├── db_utils.py\n",
    "├── langchain_utils.py\n",
    "├── pydantic_models.py\n",
    "├── requirements.txt\n",
    "└── chroma_db/  (directory for Chroma persistence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21663440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "from pydantic_models import QueryInput, QueryResponse, DocumentInfo, DeleteFileRequest\n",
    "from langchain_utils import get_rag_chain\n",
    "from db_utils import insert_application_logs, get_chat_history, get_all_documents, insert_document_record, delete_document_record\n",
    "from chroma_utils import index_document_to_chroma, delete_doc_from_chroma\n",
    "import os\n",
    "import uuid\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='app.log', level=logging.INFO)\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/chat\", response_model=QueryResponse)\n",
    "def chat(query_input: QueryInput):\n",
    "    session_id = query_input.session_id or str(uuid.uuid4())\n",
    "    logging.info(f\"Session ID: {session_id}, User Query: {query_input.question}, Model: {query_input.model.value}\")\n",
    "\n",
    "    chat_history = get_chat_history(session_id)\n",
    "    rag_chain = get_rag_chain(query_input.model.value)\n",
    "    answer = rag_chain.invoke({\n",
    "        \"input\": query_input.question,\n",
    "        \"chat_history\": chat_history\n",
    "    })['answer']\n",
    "\n",
    "    insert_application_logs(session_id, query_input.question, answer, query_input.model.value)\n",
    "    logging.info(f\"Session ID: {session_id}, AI Response: {answer}\")\n",
    "    return QueryResponse(answer=answer, session_id=session_id, model=query_input.model)\n",
    "\n",
    "@app.post(\"/upload-doc\")\n",
    "def upload_and_index_document(file: UploadFile = File(...)):\n",
    "    allowed_extensions = ['.pdf', '.docx', '.html']\n",
    "    file_extension = os.path.splitext(file.filename)[1].lower()\n",
    "\n",
    "    if file_extension not in allowed_extensions:\n",
    "        raise HTTPException(status_code=400, detail=f\"Unsupported file type. Allowed types are: {', '.join(allowed_extensions)}\")\n",
    "\n",
    "    temp_file_path = f\"temp_{file.filename}\"\n",
    "\n",
    "    try:\n",
    "        # Save the uploaded file to a temporary file\n",
    "        with open(temp_file_path, \"wb\") as buffer:\n",
    "            shutil.copyfileobj(file.file, buffer)\n",
    "\n",
    "        file_id = insert_document_record(file.filename)\n",
    "        success = index_document_to_chroma(temp_file_path, file_id)\n",
    "\n",
    "        if success:\n",
    "            return {\"message\": f\"File {file.filename} has been successfully uploaded and indexed.\", \"file_id\": file_id}\n",
    "        else:\n",
    "            delete_document_record(file_id)\n",
    "            raise HTTPException(status_code=500, detail=f\"Failed to index {file.filename}.\")\n",
    "    finally:\n",
    "        if os.path.exists(temp_file_path):\n",
    "            os.remove(temp_file_path)\n",
    "\n",
    "@app.get(\"/list-docs\", response_model=list[DocumentInfo])\n",
    "def list_documents():\n",
    "    return get_all_documents()\n",
    "\n",
    "@app.post(\"/delete-doc\")\n",
    "def delete_document(request: DeleteFileRequest):\n",
    "    chroma_delete_success = delete_doc_from_chroma(request.file_id)\n",
    "\n",
    "    if chroma_delete_success:\n",
    "        db_delete_success = delete_document_record(request.file_id)\n",
    "        if db_delete_success:\n",
    "            return {\"message\": f\"Successfully deleted document with file_id {request.file_id} from the system.\"}\n",
    "        else:\n",
    "            return {\"error\": f\"Deleted from Chroma but failed to delete document with file_id {request.file_id} from the database.\"}\n",
    "    else:\n",
    "        return {\"error\": f\"Failed to delete document with file_id {request.file_id} from Chroma.\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc4d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydantic_models.py\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "\n",
    "class ModelName(str, Enum):\n",
    "    GPT4_O = \"gpt-4o\"\n",
    "    GPT4_O_MINI = \"gpt-4o-mini\"\n",
    "\n",
    "class QueryInput(BaseModel):\n",
    "    question: str\n",
    "    session_id: str = Field(default=None)\n",
    "    model: ModelName = Field(default=ModelName.GPT4_O_MINI)\n",
    "\n",
    "class QueryResponse(BaseModel):\n",
    "    answer: str\n",
    "    session_id: str\n",
    "    model: ModelName\n",
    "\n",
    "class DocumentInfo(BaseModel):\n",
    "    id: int\n",
    "    filename: str\n",
    "    upload_timestamp: datetime\n",
    "\n",
    "class DeleteFileRequest(BaseModel):\n",
    "    file_id: int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d773d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "@app.post(\"/chat\", response_model=QueryResponse)\n",
    "def chat(query_input: QueryInput):\n",
    "    # Function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c04946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentInfo(BaseModel):\n",
    "    id: int\n",
    "    filename: str\n",
    "    upload_timestamp: datetime\n",
    "    file_size: int  # New field\n",
    "    content_type: str  # New field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#db_utils.py\n",
    "\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "DB_NAME = \"rag_app.db\"\n",
    "\n",
    "def get_db_connection():\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    conn.row_factory = sqlite3.Row\n",
    "    return conn\n",
    "\n",
    "def create_application_logs():\n",
    "    conn = get_db_connection()\n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS application_logs\n",
    "                    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                     session_id TEXT,\n",
    "                     user_query TEXT,\n",
    "                     gpt_response TEXT,\n",
    "                     model TEXT,\n",
    "                     created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
    "    conn.close()\n",
    "\n",
    "def create_document_store():\n",
    "    conn = get_db_connection()\n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS document_store\n",
    "                    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                     filename TEXT,\n",
    "                     upload_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
    "    conn.close()\n",
    "\n",
    "def insert_application_logs(session_id, user_query, gpt_response, model):\n",
    "    conn = get_db_connection()\n",
    "    conn.execute('INSERT INTO application_logs (session_id, user_query, gpt_response, model) VALUES (?, ?, ?, ?)',\n",
    "                 (session_id, user_query, gpt_response, model))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def get_chat_history(session_id):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
    "    messages = []\n",
    "    for row in cursor.fetchall():\n",
    "        messages.extend([\n",
    "            {\"role\": \"human\", \"content\": row['user_query']},\n",
    "            {\"role\": \"ai\", \"content\": row['gpt_response']}\n",
    "        ])\n",
    "    conn.close()\n",
    "    return messages\n",
    "\n",
    "def insert_document_record(filename):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('INSERT INTO document_store (filename) VALUES (?)', (filename,))\n",
    "    file_id = cursor.lastrowid\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return file_id\n",
    "\n",
    "def delete_document_record(file_id):\n",
    "    conn = get_db_connection()\n",
    "    conn.execute('DELETE FROM document_store WHERE id = ?', (file_id,))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return True\n",
    "\n",
    "def get_all_documents():\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT id, filename, upload_timestamp FROM document_store ORDER BY upload_timestamp DESC')\n",
    "    documents = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return [dict(doc) for doc in documents]\n",
    "\n",
    "\n",
    "# Initialize the database tables\n",
    "create_application_logs()\n",
    "create_document_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c87c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chroma_utils.py\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredHTMLLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "# Initialize text splitter and embedding function\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize Chroma vector store\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n",
    "\n",
    "def load_and_split_document(file_path: str) -> List[Document]:\n",
    "    if file_path.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "    elif file_path.endswith('.html'):\n",
    "        loader = UnstructuredHTMLLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_path}\")\n",
    "\n",
    "    documents = loader.load()\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "def index_document_to_chroma(file_path: str, file_id: int) -> bool:\n",
    "    try:\n",
    "        splits = load_and_split_document(file_path)\n",
    "\n",
    "        # Add metadata to each split\n",
    "        for split in splits:\n",
    "            split.metadata['file_id'] = file_id\n",
    "\n",
    "        vectorstore.add_documents(splits)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error indexing document: {e}\")\n",
    "        return False\n",
    "\n",
    "    \n",
    "def delete_doc_from_chroma(file_id: int):\n",
    "    try:\n",
    "        docs = vectorstore.get(where={\"file_id\": file_id})\n",
    "        print(f\"Found {len(docs['ids'])} document chunks for file_id {file_id}\")\n",
    "\n",
    "        vectorstore._collection.delete(where={\"file_id\": file_id})\n",
    "        print(f\"Deleted all documents with file_id {file_id}\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting document with file_id {file_id} from Chroma: {str(e)}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40312fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_utils.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "from chroma_utils import vectorstore\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", contextualize_q_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question.\"),\n",
    "    (\"system\", \"Context: {context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "def get_rag_chain(model=\"gpt-4o-mini\"):\n",
    "    llm = ChatOpenAI(model=model)\n",
    "    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)    \n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "@app.post(\"/chat\", response_model=QueryResponse)\n",
    "def chat(query_input: QueryInput):\n",
    "    # ... (other code)\n",
    "    rag_chain = get_rag_chain(query_input.model.value)\n",
    "    answer = rag_chain.invoke({\n",
    "        \"input\": query_input.question,\n",
    "        \"chat_history\": chat_history\n",
    "    })['answer']\n",
    "    # ... (rest of the function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb8b58",
   "metadata": {},
   "source": [
    "### FastAPI End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33c5ad",
   "metadata": {},
   "source": [
    "Streamlit Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836b92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlit_app.py\n",
    "\n",
    "import streamlit as st\n",
    "from sidebar import display_sidebar\n",
    "from chat_interface import display_chat_interface\n",
    "\n",
    "st.title(\"Langchain RAG Chatbot\")\n",
    "\n",
    "# Initialize session state variables\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "if \"session_id\" not in st.session_state:\n",
    "    st.session_state.session_id = None\n",
    "\n",
    "# Display the sidebar\n",
    "display_sidebar()\n",
    "\n",
    "# Display the chat interface\n",
    "display_chat_interface()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79809128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sidebar.py\n",
    "\n",
    "import streamlit as st\n",
    "from api_utils import upload_document, list_documents, delete_document\n",
    "\n",
    "def display_sidebar():\n",
    "    # Model selection\n",
    "    model_options = [\"gpt-4o\", \"gpt-4o-mini\"]\n",
    "    st.sidebar.selectbox(\"Select Model\", options=model_options, key=\"model\")\n",
    "\n",
    "    # Document upload\n",
    "    uploaded_file = st.sidebar.file_uploader(\"Choose a file\", type=[\"pdf\", \"docx\", \"html\"])\n",
    "    if uploaded_file and st.sidebar.button(\"Upload\"):\n",
    "        with st.spinner(\"Uploading...\"):\n",
    "            upload_response = upload_document(uploaded_file)\n",
    "            if upload_response:\n",
    "                st.sidebar.success(f\"File uploaded successfully with ID {upload_response['file_id']}.\")\n",
    "                st.session_state.documents = list_documents()\n",
    "\n",
    "    # List and delete documents\n",
    "    st.sidebar.header(\"Uploaded Documents\")\n",
    "    if st.sidebar.button(\"Refresh Document List\"):\n",
    "        st.session_state.documents = list_documents()\n",
    "\n",
    "    # Display document list and delete functionality\n",
    "    if \"documents\" in st.session_state and st.session_state.documents:\n",
    "        for doc in st.session_state.documents:\n",
    "            st.sidebar.text(f\"{doc['filename']} (ID: {doc['id']})\")\n",
    "\n",
    "        selected_file_id = st.sidebar.selectbox(\"Select a document to delete\", \n",
    "                                                options=[doc['id'] for doc in st.session_state.documents])\n",
    "        if st.sidebar.button(\"Delete Selected Document\"):\n",
    "            delete_response = delete_document(selected_file_id)\n",
    "            if delete_response:\n",
    "                st.sidebar.success(f\"Document deleted successfully.\")\n",
    "                st.session_state.documents = list_documents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af8d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat_interface.py\n",
    "\n",
    "import streamlit as st\n",
    "from api_utils import get_api_response\n",
    "\n",
    "def display_chat_interface():\n",
    "    # Display chat history\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    # Handle new user input\n",
    "    if prompt := st.chat_input(\"Query:\"):\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(prompt)\n",
    "\n",
    "        # Get API response\n",
    "        with st.spinner(\"Generating response...\"):\n",
    "            response = get_api_response(prompt, st.session_state.session_id, st.session_state.model)\n",
    "\n",
    "            if response:\n",
    "                st.session_state.session_id = response.get('session_id')\n",
    "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response['answer']})\n",
    "\n",
    "                with st.chat_message(\"assistant\"):\n",
    "                    st.markdown(response['answer'])\n",
    "\n",
    "                with st.expander(\"Details\"):\n",
    "                    st.subheader(\"Generated Answer\")\n",
    "                    st.code(response['answer'])\n",
    "                    st.subheader(\"Model Used\")\n",
    "                    st.code(response['model'])\n",
    "                    st.subheader(\"Session ID\")\n",
    "                    st.code(response['session_id'])\n",
    "            else:\n",
    "                st.error(\"Failed to get a response from the API. Please try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a28e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_utils.py\n",
    "\n",
    "import requests\n",
    "import streamlit as st\n",
    "\n",
    "def get_api_response(question, session_id, model):\n",
    "    headers = {'accept': 'application/json', 'Content-Type': 'application/json'}\n",
    "    data = {\"question\": question, \"model\": model}\n",
    "    if session_id:\n",
    "        data[\"session_id\"] = session_id\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\"http://localhost:8000/chat\", headers=headers, json=data)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            st.error(f\"API request failed with status code {response.status_code}: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def upload_document(file):\n",
    "    try:\n",
    "        files = {\"file\": (file.name, file, file.type)}\n",
    "        response = requests.post(\"http://localhost:8000/upload-doc\", files=files)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            st.error(f\"Failed to upload file. Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred while uploading the file: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def list_documents():\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:8000/list-docs\")\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            st.error(f\"Failed to fetch document list. Error: {response.status_code} - {response.text}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred while fetching the document list: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def delete_document(file_id):\n",
    "    headers = {'accept': 'application/json', 'Content-Type': 'application/json'}\n",
    "    data = {\"file_id\": file_id}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\"http://localhost:8000/delete-doc\", headers=headers, json=data)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            st.error(f\"Failed to delete document. Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred while deleting the document: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4550b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.futuresmart.ai/langchain-rag-from-basics-to-production-ready-rag-chatbot\n",
    "# https://blog.futuresmart.ai/building-a-production-ready-rag-chatbot-with-fastapi-and-langchain\n",
    "# https://blog.futuresmart.ai/building-a-user-friendly-interface-with-streamlit-for-our-rag-chatbot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
